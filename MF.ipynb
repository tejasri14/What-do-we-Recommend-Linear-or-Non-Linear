{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIePi9k8Xz8M",
    "outputId": "1b3af0e1-75c9-46b6-a7f9-2b644a91599d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-04 00:14:30--  https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip [following]\n",
      "--2022-05-04 00:14:30--  https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15312323 (15M) [application/zip]\n",
      "Saving to: ‘recsys.zip’\n",
      "\n",
      "recsys.zip          100%[===================>]  14.60M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-05-04 00:14:31 (132 MB/s) - ‘recsys.zip’ saved [15312323/15312323]\n",
      "\n",
      "Archive:  recsys.zip\n",
      "   creating: recsys/\n",
      "  inflating: recsys/datasets.py      \n",
      "  inflating: recsys/preprocessing.py  \n",
      "  inflating: recsys/utils.py         \n",
      "  inflating: recsys/requirements.txt  \n",
      "   creating: recsys/.vscode/\n",
      "  inflating: recsys/.vscode/settings.json  \n",
      "   creating: recsys/__pycache__/\n",
      "  inflating: recsys/__pycache__/datasets.cpython-36.pyc  \n",
      "  inflating: recsys/__pycache__/datasets.cpython-37.pyc  \n",
      "  inflating: recsys/__pycache__/utils.cpython-36.pyc  \n",
      "  inflating: recsys/__pycache__/preprocessing.cpython-37.pyc  \n",
      "  inflating: recsys/__pycache__/datasets.cpython-38.pyc  \n",
      "  inflating: recsys/__pycache__/preprocessing.cpython-36.pyc  \n",
      "  inflating: recsys/__pycache__/preprocessing.cpython-38.pyc  \n",
      "   creating: recsys/memories/\n",
      "  inflating: recsys/memories/ItemToItem.py  \n",
      "  inflating: recsys/memories/UserToUser.py  \n",
      "   creating: recsys/memories/__pycache__/\n",
      "  inflating: recsys/memories/__pycache__/UserToUser.cpython-36.pyc  \n",
      "  inflating: recsys/memories/__pycache__/UserToUser.cpython-37.pyc  \n",
      "  inflating: recsys/memories/__pycache__/ItemToItem.cpython-37.pyc  \n",
      "  inflating: recsys/memories/__pycache__/user2user.cpython-36.pyc  \n",
      "  inflating: recsys/memories/__pycache__/ItemToItem.cpython-36.pyc  \n",
      "   creating: recsys/models/\n",
      "  inflating: recsys/models/SVD.py    \n",
      "  inflating: recsys/models/MatrixFactorization.py  \n",
      "  inflating: recsys/models/ExplainableMF.py  \n",
      "  inflating: recsys/models/NonnegativeMF.py  \n",
      "   creating: recsys/models/__pycache__/\n",
      "  inflating: recsys/models/__pycache__/SVD.cpython-36.pyc  \n",
      "  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-37.pyc  \n",
      "  inflating: recsys/models/__pycache__/ExplainableMF.cpython-36.pyc  \n",
      "  inflating: recsys/models/__pycache__/ExplainableMF.cpython-37.pyc  \n",
      "  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-36.pyc  \n",
      "   creating: recsys/metrics/\n",
      "  inflating: recsys/metrics/EvaluationMetrics.py  \n",
      "   creating: recsys/img/\n",
      "  inflating: recsys/img/MF-and-NNMF.png  \n",
      "  inflating: recsys/img/svd.png      \n",
      "  inflating: recsys/img/MF.png       \n",
      "   creating: recsys/predictions/\n",
      "   creating: recsys/predictions/item2item/\n",
      "   creating: recsys/weights/\n",
      "   creating: recsys/weights/item2item/\n",
      "   creating: recsys/weights/item2item/ml1m/\n",
      "  inflating: recsys/weights/item2item/ml1m/similarities.npy  \n",
      "  inflating: recsys/weights/item2item/ml1m/neighbors.npy  \n",
      "   creating: recsys/weights/item2item/ml100k/\n",
      "  inflating: recsys/weights/item2item/ml100k/similarities.npy  \n",
      "  inflating: recsys/weights/item2item/ml100k/neighbors.npy  \n"
     ]
    }
   ],
   "source": [
    "# the part of the code is taken from https://github.com/nzhinusoftcm/review-on-collaborative-filtering\n",
    "\n",
    "import os\n",
    "if not (os.path.exists(\"recsys.zip\") or os.path.exists(\"recsys\")):\n",
    "    !wget https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip    \n",
    "    !unzip recsys.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwqEgWPuawkK"
   },
   "outputs": [],
   "source": [
    "#importing the datasets and libraries\n",
    "from recsys.preprocessing import mean_ratings\n",
    "from recsys.preprocessing import normalized_ratings\n",
    "from recsys.preprocessing import ids_encoder\n",
    "from recsys.preprocessing import train_test_split\n",
    "from recsys.preprocessing import rating_matrix\n",
    "from recsys.preprocessing import get_examples\n",
    "from recsys.preprocessing import scale_ratings\n",
    "\n",
    "from recsys.datasets import ml100k\n",
    "from recsys.datasets import ml1m\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbp1frB7azQX"
   },
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    \n",
    "    def __init__(self, m, n, k=10, alpha=0.001, lamb=0.01):\n",
    "\n",
    "        np.random.seed(32)\n",
    "        \n",
    "        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt\n",
    "        self.k = k\n",
    "        self.P = np.random.normal(size=(m, k))\n",
    "        self.Q = np.random.normal(size=(n, k))\n",
    "        \n",
    "        # hyperparameter initialization\n",
    "        self.alpha = alpha\n",
    "        self.lamb = lamb\n",
    "        \n",
    "        # training history\n",
    "        self.history = {\n",
    "            \"epochs\":[],\n",
    "            \"loss\":[],\n",
    "            \"val_loss\":[],\n",
    "            \"lr\":[]\n",
    "        }\n",
    "    \n",
    "    def print_training_parameters(self):\n",
    "        print('Training Matrix Factorization Model ...')\n",
    "        print(f'k={self.k} \\t alpha={self.alpha} \\t lambda={self.lamb}')\n",
    "    \n",
    "    def update_rule(self, u, i, error):\n",
    "        self.P[u] = self.P[u] + self.alpha * (error * self.Q[i] - self.lamb * self.P[u])\n",
    "        self.Q[i] = self.Q[i] + self.alpha * (error * self.P[u] - self.lamb * self.Q[i])\n",
    "        \n",
    "    def mae(self,  x_train, y_train):\n",
    "        \"\"\"\n",
    "        returns the Mean Absolute Error\n",
    "        \"\"\"\n",
    "        # number of training exemples\n",
    "        M = x_train.shape[0]\n",
    "        error = 0\n",
    "        for pair, r in zip(x_train, y_train):\n",
    "            u, i = pair\n",
    "            error += abs(r - np.dot(self.P[u], self.Q[i]))\n",
    "        return error/M\n",
    "    \n",
    "    def print_training_progress(self, epoch, epochs, error, val_error, steps=5):\n",
    "        if epoch == 1 or epoch % steps == 0 :\n",
    "                print(\"epoch {}/{} - loss : {} - val_loss : {}\".format(epoch, epochs, round(error,3), round(val_error,3)))\n",
    "                \n",
    "    def learning_rate_schedule(self, epoch, target_epochs = 20):\n",
    "        if (epoch >= target_epochs) and (epoch % target_epochs == 0):\n",
    "                factor = epoch // target_epochs\n",
    "                self.alpha = self.alpha * (1 / (factor * 20))\n",
    "                print(\"\\nLearning Rate : {}\\n\".format(self.alpha))\n",
    "    \n",
    "    def fit(self, x_train, y_train, validation_data, epochs=1000):\n",
    "        self.print_training_parameters()\n",
    "        \n",
    "        # validation data\n",
    "        x_test, y_test = validation_data\n",
    "        \n",
    "        # loop over the number of epochs\n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            # for each pair (u,i) and the corresponding rating r\n",
    "            for pair, r in zip(x_train, y_train):\n",
    "                \n",
    "                # get encoded values of userid and itemid from pair\n",
    "                u,i = pair\n",
    "                \n",
    "                # compute the predicted rating r_hat\n",
    "                r_hat = np.dot(self.P[u], self.Q[i])\n",
    "                \n",
    "                # compute the prediction error\n",
    "                e = abs(r - r_hat)\n",
    "                \n",
    "                # update rules\n",
    "                self.update_rule(u, i, e)\n",
    "                \n",
    "            # training and validation error  after this epochs\n",
    "            error = self.mae(x_train, y_train)\n",
    "            val_error = self.mae(x_test, y_test)\n",
    "            \n",
    "            # update history\n",
    "            self.history['epochs'].append(epoch)\n",
    "            self.history['loss'].append(error)\n",
    "            self.history['val_loss'].append(val_error)\n",
    "            \n",
    "            # update history\n",
    "            self.update_history(epoch, error, val_error)\n",
    "            \n",
    "            # print training progress after each steps epochs\n",
    "            self.print_training_progress(epoch, epochs, error, val_error, steps=1)\n",
    "              \n",
    "            # leaning rate scheduler : redure the learning rate as we go deeper in the number of epochs\n",
    "            # self.learning_rate_schedule(epoch)\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def update_history(self, epoch, error, val_error):\n",
    "        self.history['epochs'].append(epoch)\n",
    "        self.history['loss'].append(error)\n",
    "        self.history['val_loss'].append(val_error)\n",
    "        self.history['lr'].append(self.alpha)\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "\n",
    "        error = self.mae(x_test, y_test)\n",
    "        print(f\"validation error : {round(error,3)}\")\n",
    "        \n",
    "        return error\n",
    "      \n",
    "    def predict(self, userid, itemid):\n",
    "\n",
    "        # encode user and item ids to be able to access their latent factors in\n",
    "        # matrices P and Q\n",
    "        u = uencoder.transform([userid])[0]\n",
    "        i = iencoder.transform([itemid])[0]\n",
    "\n",
    "        # rating prediction using encoded ids. Dot product between P_u and Q_i\n",
    "        r = np.dot(self.P[u], self.Q[i])\n",
    "        return r\n",
    "\n",
    "    def recommend(self, userid, N=30):\n",
    "\n",
    "        # encode the userid\n",
    "        u = uencoder.transform([userid])[0]\n",
    "\n",
    "        # predictions for users userid on all product\n",
    "        predictions = np.dot(self.P[u], self.Q.T)\n",
    "\n",
    "        # get the indices of the top N predictions\n",
    "        top_idx = np.flip(np.argsort(predictions))[:N]\n",
    "\n",
    "        # decode indices to get their corresponding itemids\n",
    "        top_items = iencoder.inverse_transform(top_idx)\n",
    "\n",
    "        # take corresponding predictions for top N indices\n",
    "        preds = predictions[top_idx]\n",
    "\n",
    "        return top_items, preds   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkYesogda2zH"
   },
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntzuCVJMa5iw",
    "outputId": "d4f567c1-b4d9-41e1-f3da-5f45603aa105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download data 100.2%\n",
      "Successfully downloaded ml-100k.zip 4924029 bytes.\n",
      "Unzipping the ml-100k.zip zip file ...\n"
     ]
    }
   ],
   "source": [
    "# load the ml100k dataset\n",
    "ratings, movies = ml100k.load()\n",
    "\n",
    "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
    "\n",
    "m = ratings.userid.nunique()   # total number of users\n",
    "n = ratings.itemid.nunique()   # total number of items\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings)\n",
    "\n",
    "# train test split\n",
    "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Th6cAAD2a8Aw",
    "outputId": "6c0e3cf9-43d8-4e7c-d8cb-0944a3f7d61f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/10 - loss : 2.734 - val_loss : 2.779\n",
      "epoch 2/10 - loss : 1.764 - val_loss : 1.794\n",
      "epoch 3/10 - loss : 1.592 - val_loss : 1.614\n",
      "epoch 4/10 - loss : 1.538 - val_loss : 1.556\n",
      "epoch 5/10 - loss : 1.515 - val_loss : 1.531\n",
      "epoch 6/10 - loss : 1.503 - val_loss : 1.517\n",
      "epoch 7/10 - loss : 1.496 - val_loss : 1.509\n",
      "epoch 8/10 - loss : 1.491 - val_loss : 1.504\n",
      "epoch 9/10 - loss : 1.488 - val_loss : 1.5\n",
      "epoch 10/10 - loss : 1.486 - val_loss : 1.497\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LY3a5-XCa-gN",
    "outputId": "f05852c8-c446-4b4b-cc1e-0dedd38cf202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 1.497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4973507972141993"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MF.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTU4-glFbA-W"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings, movies = ml100k.load()\n",
    "\n",
    "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
    "\n",
    "m = ratings['userid'].nunique()   # total number of users\n",
    "n = ratings['itemid'].nunique()   # total number of items\n",
    "\n",
    "# normalize ratings by substracting means\n",
    "normalized_column_name = \"norm_rating\"\n",
    "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
    "\n",
    "# train test split\n",
    "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eB4EPq9dbFiP",
    "outputId": "1f43a21c-9453-4cd5-e116-5dfe867f8db1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorization Model ...\n",
      "k=10 \t alpha=0.01 \t lambda=1.5\n",
      "epoch 1/10 - loss : 0.851 - val_loss : 0.847\n",
      "epoch 2/10 - loss : 0.831 - val_loss : 0.831\n",
      "epoch 3/10 - loss : 0.828 - val_loss : 0.829\n",
      "epoch 4/10 - loss : 0.827 - val_loss : 0.828\n",
      "epoch 5/10 - loss : 0.827 - val_loss : 0.828\n",
      "epoch 6/10 - loss : 0.826 - val_loss : 0.828\n",
      "epoch 7/10 - loss : 0.826 - val_loss : 0.828\n",
      "epoch 8/10 - loss : 0.826 - val_loss : 0.828\n",
      "epoch 9/10 - loss : 0.826 - val_loss : 0.828\n",
      "epoch 10/10 - loss : 0.826 - val_loss : 0.828\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
    "\n",
    "# fit the model on the training set\n",
    "history = MF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qD3l75X6bILf",
    "outputId": "9d39c383-4c30-407e-d897-7896be382a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 0.828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8276982643684648"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
