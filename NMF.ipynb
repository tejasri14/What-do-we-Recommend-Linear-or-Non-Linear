{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AyuetGsr0BO",
    "outputId": "1dd57329-6dff-4527-f4ff-7e50c8162292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-03 22:33:21--  https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip\n",
      "Resolving github.com (github.com)... 140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip [following]\n",
      "--2022-05-03 22:33:21--  https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15312323 (15M) [application/zip]\n",
      "Saving to: ‘recsys.zip’\n",
      "\n",
      "recsys.zip          100%[===================>]  14.60M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-05-03 22:33:22 (140 MB/s) - ‘recsys.zip’ saved [15312323/15312323]\n",
      "\n",
      "Archive:  recsys.zip\n",
      "   creating: recsys/\n",
      "  inflating: recsys/datasets.py      \n",
      "  inflating: recsys/preprocessing.py  \n",
      "  inflating: recsys/utils.py         \n",
      "  inflating: recsys/requirements.txt  \n",
      "   creating: recsys/.vscode/\n",
      "  inflating: recsys/.vscode/settings.json  \n",
      "   creating: recsys/__pycache__/\n",
      "  inflating: recsys/__pycache__/datasets.cpython-36.pyc  \n",
      "  inflating: recsys/__pycache__/datasets.cpython-37.pyc  \n",
      "  inflating: recsys/__pycache__/utils.cpython-36.pyc  \n",
      "  inflating: recsys/__pycache__/preprocessing.cpython-37.pyc  \n",
      "  inflating: recsys/__pycache__/datasets.cpython-38.pyc  \n",
      "  inflating: recsys/__pycache__/preprocessing.cpython-36.pyc  \n",
      "  inflating: recsys/__pycache__/preprocessing.cpython-38.pyc  \n",
      "   creating: recsys/memories/\n",
      "  inflating: recsys/memories/ItemToItem.py  \n",
      "  inflating: recsys/memories/UserToUser.py  \n",
      "   creating: recsys/memories/__pycache__/\n",
      "  inflating: recsys/memories/__pycache__/UserToUser.cpython-36.pyc  \n",
      "  inflating: recsys/memories/__pycache__/UserToUser.cpython-37.pyc  \n",
      "  inflating: recsys/memories/__pycache__/ItemToItem.cpython-37.pyc  \n",
      "  inflating: recsys/memories/__pycache__/user2user.cpython-36.pyc  \n",
      "  inflating: recsys/memories/__pycache__/ItemToItem.cpython-36.pyc  \n",
      "   creating: recsys/models/\n",
      "  inflating: recsys/models/SVD.py    \n",
      "  inflating: recsys/models/MatrixFactorization.py  \n",
      "  inflating: recsys/models/ExplainableMF.py  \n",
      "  inflating: recsys/models/NonnegativeMF.py  \n",
      "   creating: recsys/models/__pycache__/\n",
      "  inflating: recsys/models/__pycache__/SVD.cpython-36.pyc  \n",
      "  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-37.pyc  \n",
      "  inflating: recsys/models/__pycache__/ExplainableMF.cpython-36.pyc  \n",
      "  inflating: recsys/models/__pycache__/ExplainableMF.cpython-37.pyc  \n",
      "  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-36.pyc  \n",
      "   creating: recsys/metrics/\n",
      "  inflating: recsys/metrics/EvaluationMetrics.py  \n",
      "   creating: recsys/img/\n",
      "  inflating: recsys/img/MF-and-NNMF.png  \n",
      "  inflating: recsys/img/svd.png      \n",
      "  inflating: recsys/img/MF.png       \n",
      "   creating: recsys/predictions/\n",
      "   creating: recsys/predictions/item2item/\n",
      "   creating: recsys/weights/\n",
      "   creating: recsys/weights/item2item/\n",
      "   creating: recsys/weights/item2item/ml1m/\n",
      "  inflating: recsys/weights/item2item/ml1m/similarities.npy  \n",
      "  inflating: recsys/weights/item2item/ml1m/neighbors.npy  \n",
      "   creating: recsys/weights/item2item/ml100k/\n",
      "  inflating: recsys/weights/item2item/ml100k/similarities.npy  \n",
      "  inflating: recsys/weights/item2item/ml100k/neighbors.npy  \n"
     ]
    }
   ],
   "source": [
    "# some functions in this code are taken from https://github.com/nzhinusoftcm/review-on-collaborative-filtering\n",
    "\n",
    "import os\n",
    "if not (os.path.exists(\"recsys.zip\") or os.path.exists(\"recsys\")):\n",
    "    !wget https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip    \n",
    "    !unzip recsys.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TxbJNv1hD97U",
    "outputId": "615426c3-9ddd-432e-b6fc-36521346e7c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting recommenders\n",
      "  Downloading recommenders-1.1.0-py3-none-manylinux1_x86_64.whl (335 kB)\n",
      "\u001b[K     |████████████████████████████████| 335 kB 7.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.4.1)\n",
      "Collecting lightfm<2,>=1.15\n",
      "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
      "\u001b[K     |████████████████████████████████| 310 kB 40.8 MB/s \n",
      "\u001b[?25hCollecting pandera[strategies]>=0.6.5\n",
      "  Downloading pandera-0.9.0-py3-none-any.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 37.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: jinja2<3.1,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.11.3)\n",
      "Collecting nltk<4,>=3.4\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 40.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.64.0)\n",
      "Requirement already satisfied: bottleneck<2,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.4)\n",
      "Collecting category-encoders<2,>=1.3.0\n",
      "  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 7.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.0.2)\n",
      "Requirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.2.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.23.0)\n",
      "Requirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.5)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Requirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.11.2)\n",
      "Requirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.51.2)\n",
      "Collecting transformers<5,>=2.5.0\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 35.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.2.2)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.21.6)\n",
      "Collecting pyyaml<6,>=5.4.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 49.0 MB/s \n",
      "\u001b[?25hCollecting cornac<2,>=1.1.2\n",
      "  Downloading cornac-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 18.3 MB/s \n",
      "\u001b[?25hCollecting memory-profiler<1,>=0.54.0\n",
      "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
      "Collecting scikit-surprise>=1.0.6\n",
      "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 32.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.10.2)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.2)\n",
      "Collecting powerlaw\n",
      "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3.1,>=2->recommenders) (2.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (1.4.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4,>=2.2.2->recommenders) (4.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.4.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (1.1.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "\u001b[K     |████████████████████████████████| 749 kB 48.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (7.1.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (0.34.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (57.4.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders) (2022.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.14.0)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.9 MB 36.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (6.0.1)\n",
      "Collecting typing-inspect>=0.6.0\n",
      "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (21.3)\n",
      "Collecting hypothesis>=5.41.1\n",
      "  Downloading hypothesis-6.46.2-py3-none-any.whl (383 kB)\n",
      "\u001b[K     |████████████████████████████████| 383 kB 51.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (2.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->category-encoders<2,>=1.3.0->recommenders) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders) (3.1.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 6.5 MB/s \n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 18.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (4.11.3)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 74.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (3.6.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders) (3.8.0)\n",
      "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.2.1)\n",
      "Building wheels for collected packages: lightfm, memory-profiler, retrying, scikit-surprise, sacremoses\n",
      "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705375 sha256=97b4db8c567d7759ee4a7c594abd3381e54ccb886df9f3d0e9c87a774b2b25ea\n",
      "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
      "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=706d741090d118c6c95352eb409b2db64e1e8e456a4e66fe26e01f58e4937741\n",
      "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
      "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11447 sha256=f4df9fbcedc3c9336fd398ad08ea6b6a51f851fa207c50d6ad8bf3e0a6f4fca1\n",
      "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
      "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1630135 sha256=5c5b66f1ef9ed2812f957b15b294953b6db9edac05c11b76b3023ec6a42accd5\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=14f5541c749036c23a83550d30336ed496b7f3477ca9122b2d23a1e87bd548d4\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
      "Successfully built lightfm memory-profiler retrying scikit-surprise sacremoses\n",
      "Installing collected packages: mypy-extensions, typing-inspect, regex, pyyaml, pydantic, tokenizers, sacremoses, powerlaw, pandera, hypothesis, huggingface-hub, transformers, scikit-surprise, retrying, nltk, memory-profiler, lightfm, cornac, category-encoders, recommenders\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2019.12.20\n",
      "    Uninstalling regex-2019.12.20:\n",
      "      Successfully uninstalled regex-2019.12.20\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed category-encoders-1.3.0 cornac-1.14.2 huggingface-hub-0.5.1 hypothesis-6.46.2 lightfm-1.16 memory-profiler-0.60.0 mypy-extensions-0.4.3 nltk-3.7 pandera-0.9.0 powerlaw-1.5 pydantic-1.9.0 pyyaml-5.4.1 recommenders-1.1.0 regex-2022.4.24 retrying-1.3.3 sacremoses-0.0.53 scikit-surprise-1.1.1 tokenizers-0.12.1 transformers-4.18.0 typing-inspect-0.7.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "yaml"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NpF6Xi6Gtu9",
    "outputId": "5fdcb0fd-ad1a-4f39-fd1f-c6c16ee1d626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf_slim\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█                               | 10 kB 21.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 20 kB 24.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 30 kB 29.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 40 kB 12.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 51 kB 10.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 61 kB 11.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 71 kB 9.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▌                        | 81 kB 7.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 92 kB 8.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 102 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 112 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 122 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 133 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 143 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 153 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 163 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 174 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 184 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 194 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 204 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 215 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 225 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 235 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 245 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 256 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 266 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 276 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 286 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 296 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 307 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 317 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 327 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 337 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 348 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 352 kB 8.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (1.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
      "Installing collected packages: tf-slim\n",
      "Successfully installed tf-slim-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3VNg1p0DmaD"
   },
   "outputs": [],
   "source": [
    "#importing the datasets and libraries\n",
    "from recsys.preprocessing import mean_ratings\n",
    "from recsys.preprocessing import normalized_ratings\n",
    "from recsys.preprocessing import ids_encoder\n",
    "from recsys.preprocessing import train_test_split\n",
    "from recsys.preprocessing import rating_matrix\n",
    "from recsys.preprocessing import get_examples\n",
    "from recsys.preprocessing import scale_ratings\n",
    "\n",
    "from recsys.datasets import ml1m\n",
    "from recsys.datasets import ml100k\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrNi6e0wIyN3",
    "outputId": "0eff4dd5-d3d5-4cd0-ed13-66f0b4348a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download data 100.2%\n",
      "Successfully downloaded ml-100k.zip 4924029 bytes.\n",
      "Unzipping the ml-100k.zip zip file ...\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "ratings, movies = ml100k.load()\n",
    "\n",
    "# prepare data\n",
    "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
    "\n",
    "# convert ratings from dataframe to numpy array\n",
    "np_ratings = ratings.to_numpy()\n",
    "\n",
    "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
    "raw_examples, raw_labels = get_examples(ratings, labels_column=\"rating\")\n",
    "\n",
    "# train test split\n",
    "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hlHKcWgIySn"
   },
   "outputs": [],
   "source": [
    "class NMF:\n",
    "    \n",
    "    def __init__(self, ratings, m, n, uencoder, iencoder, K=10, lambda_P=0.01, lambda_Q=0.01):\n",
    "        \n",
    "        np.random.seed(32)\n",
    "        \n",
    "        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt\n",
    "        self.ratings = ratings\n",
    "        self.np_ratings = ratings.to_numpy()\n",
    "        self.K = K\n",
    "        self.P = np.random.rand(m, K)\n",
    "        self.Q = np.random.rand(n, K)\n",
    "        \n",
    "        # hyper parameter initialization\n",
    "        self.lambda_P = lambda_P\n",
    "        self.lambda_Q = lambda_Q\n",
    "\n",
    "        # initialize encoders\n",
    "        self.uencoder = uencoder\n",
    "        self.iencoder = iencoder\n",
    "        \n",
    "        # training history\n",
    "        self.history = {\n",
    "            \"epochs\": [],\n",
    "            \"loss\": [],\n",
    "            \"val_loss\": [],\n",
    "        }\n",
    "    \n",
    "    def print_training_parameters(self):\n",
    "        print('Training NMF ...')\n",
    "        print(f'k={self.K}')\n",
    "        \n",
    "    def mae(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        returns the Mean Absolute Error\n",
    "        \"\"\"\n",
    "        # number of training examples\n",
    "        m = x_train.shape[0]\n",
    "        error = 0\n",
    "        for pair, r in zip(x_train, y_train):\n",
    "            u, i = pair\n",
    "            error += abs(r - np.dot(self.P[u], self.Q[i]))\n",
    "        return error / m\n",
    "    \n",
    "    def update_rule(self, u, i, error):\n",
    "        I = self.np_ratings[self.np_ratings[:, 0] == u][:, [1, 2]]\n",
    "        U = self.np_ratings[self.np_ratings[:, 1] == i][:, [0, 2]]    \n",
    "                    \n",
    "        num = self.P[u] * np.dot(self.Q[I[:, 0]].T, I[:, 1])\n",
    "        dem = np.dot(self.Q[I[:, 0]].T, np.dot(self.P[u], self.Q[I[:, 0]].T)) + self.lambda_P * len(I) * self.P[u]\n",
    "        self.P[u] = num / dem\n",
    "\n",
    "        num = self.Q[i] * np.dot(self.P[U[:, 0]].T, U[:, 1])\n",
    "        dem = np.dot(self.P[U[:, 0]].T, np.dot(self.P[U[:, 0]], self.Q[i].T)) + self.lambda_Q * len(U) * self.Q[i]\n",
    "        self.Q[i] = num / dem\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_training_progress(epoch, epochs, error, val_error, steps=5):\n",
    "        if epoch == 1 or epoch % steps == 0:\n",
    "            print(f\"epoch {epoch}/{epochs} - loss : {round(error, 3)} - val_loss : {round(val_error, 3)}\")\n",
    "                \n",
    "    def fit(self, x_train, y_train, validation_data, epochs=10):\n",
    "\n",
    "        self.print_training_parameters()\n",
    "        x_test, y_test = validation_data\n",
    "        for epoch in range(1, epochs+1):\n",
    "            for pair, r in zip(x_train, y_train):\n",
    "                u, i = pair\n",
    "                r_hat = np.dot(self.P[u], self.Q[i])\n",
    "                e = abs(r - r_hat)\n",
    "                self.update_rule(u, i, e)                \n",
    "            # training and validation error  after this epochs\n",
    "            error = self.mae(x_train, y_train)\n",
    "            val_error = self.mae(x_test, y_test)\n",
    "            self.update_history(epoch, error, val_error)\n",
    "            self.print_training_progress(epoch, epochs, error, val_error, steps=1)\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def update_history(self, epoch, error, val_error):\n",
    "        self.history['epochs'].append(epoch)\n",
    "        self.history['loss'].append(error)\n",
    "        self.history['val_loss'].append(val_error)\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):        \n",
    "        error = self.mae(x_test, y_test)\n",
    "        print(f\"validation error : {round(error,3)}\")\n",
    "        print('MAE : ', error)        \n",
    "        return error\n",
    "      \n",
    "    def predict(self, userid, itemid):\n",
    "        u = self.uencoder.transform([userid])[0]\n",
    "        i = self.iencoder.transform([itemid])[0]\n",
    "        r = np.dot(self.P[u], self.Q[i])\n",
    "        return r\n",
    "\n",
    "    def recommend(self, userid, N=30):\n",
    "        # encode the userid\n",
    "        u = self.uencoder.transform([userid])[0]\n",
    "\n",
    "        # predictions for users userid on all product\n",
    "        predictions = np.dot(self.P[u], self.Q.T)\n",
    "\n",
    "        # get the indices of the top N predictions\n",
    "        top_idx = np.flip(np.argsort(predictions))[:N]\n",
    "\n",
    "        # decode indices to get their corresponding itemids\n",
    "        top_items = self.iencoder.inverse_transform(top_idx)\n",
    "\n",
    "        # take corresponding predictions for top N indices\n",
    "        preds = predictions[top_idx]\n",
    "\n",
    "        return top_items, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_aO5jz7yIyWt",
    "outputId": "6164ea00-263b-40f7-dfae-e946edc71d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NMF ...\n",
      "k=10\n",
      "epoch 1/10 - loss : 0.916 - val_loss : 0.917\n",
      "epoch 2/10 - loss : 0.915 - val_loss : 0.917\n",
      "epoch 3/10 - loss : 0.915 - val_loss : 0.917\n",
      "epoch 4/10 - loss : 0.915 - val_loss : 0.917\n",
      "epoch 5/10 - loss : 0.915 - val_loss : 0.917\n",
      "epoch 6/10 - loss : 0.915 - val_loss : 0.917\n",
      "epoch 7/10 - loss : 0.915 - val_loss : 0.917\n",
      "epoch 8/10 - loss : 0.915 - val_loss : 0.917\n",
      "epoch 9/10 - loss : 0.915 - val_loss : 0.917\n",
      "epoch 10/10 - loss : 0.915 - val_loss : 0.917\n"
     ]
    }
   ],
   "source": [
    "m = ratings['userid'].nunique()   # total number of users\n",
    "n = ratings['itemid'].nunique()   # total number of items\n",
    "\n",
    "# create and train the model\n",
    "nmf = NMF(ratings, m, n, uencoder, iencoder, K=10, lambda_P=0.6, lambda_Q=0.6)\n",
    "history = nmf.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJ679kDIIyao",
    "outputId": "6e172b19-72f6-4717-83d7-66bc29851905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error : 0.917\n",
      "MAE :  0.9165041343019539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9165041343019539"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLU_V707IygN",
    "outputId": "5d6b51e1-8168-4347-ddd6-c53f955ffb0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ml-100k could not be found. Do you want to download it? [Y/n] Y\n",
      "Trying to download dataset from http://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
      "Done! Dataset ml-100k has been saved to /root/.surprise_data/ml-100k\n",
      "Evaluating MAE of algorithm NMF on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     0.9626  0.9435  0.9777  0.9607  0.9510  0.9591  0.0116  \n",
      "Fit time          0.97    0.96    1.11    1.00    1.10    1.03    0.06    \n",
      "Test time         0.16    0.22    0.14    0.14    0.35    0.20    0.08    \n"
     ]
    }
   ],
   "source": [
    "from surprise import NMF\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed).\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Use the NMF algorithm.\n",
    "nmf = NMF(n_factors=10, n_epochs=10)\n",
    "\n",
    "# Run 5-fold cross-validation and print results.\n",
    "history = cross_validate(nmf, data, measures=['MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E7MtmbUkJOuB",
    "outputId": "e0e6ffff-52f8-4321-f2f3-5af2b224b440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ml-1m could not be found. Do you want to download it? [Y/n] Y\n",
      "Trying to download dataset from http://files.grouplens.org/datasets/movielens/ml-1m.zip...\n",
      "Done! Dataset ml-1m has been saved to /root/.surprise_data/ml-1m\n",
      "Evaluating MAE of algorithm NMF on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     0.9559  0.9580  0.9566  0.9639  0.9490  0.9567  0.0047  \n",
      "Fit time          9.46    10.29   10.05   10.55   10.65   10.20   0.43    \n",
      "Test time         2.82    2.16    2.30    2.10    2.58    2.39    0.27    \n"
     ]
    }
   ],
   "source": [
    "data = Dataset.load_builtin('ml-1m')\n",
    "nmf = NMF(n_factors=10, n_epochs=10)\n",
    "history = cross_validate(nmf, data, measures=['MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0aUw4IDJR37"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NMF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
